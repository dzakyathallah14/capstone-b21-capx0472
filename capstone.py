# -*- coding: utf-8 -*-
"""Copy of Copy OF CaPSTONE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wyIaHXV2G02Tv6-courGA34G9mhXcPXl
"""

!pip install mtcnn
import tensorflow as tf
from tensorflow.keras.models import Model, Sequential
import os 
import pandas as pd
import cv2
import mtcnn
import pickle 
import numpy as np 
from sklearn.preprocessing import Normalizer
from tensorflow.keras.models import load_model
from scipy.spatial.distance import cosine
from tensorflow.keras.models import load_model
import shutil
from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img

#informasi path di gcloud dan local

#local directory
path_train = '/content/train'
path_test = '/content/test'

def scaling(x, scale):
    return x * scale
model = tf.keras.Sequential([
    tf.keras.applications.MobileNetV2(input_shape=(160, 160, 3),
                                               include_top=False,
                                               weights='imagenet'),
    tf.keras.layers.Conv2D(160, 3, activation='relu'),  # 2
    tf.keras.layers.Conv2D(128, 3, activation='relu'),  # 2
    tf.keras.layers.Dropout(0.2),  # 3
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.2),  # 3
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.GlobalAveragePooling2D(),  # 4
    tf.keras.layers.Dense(1, activation='softmax')  # 5
])

model.compile(optimizer="adam",  # 1
              loss='CategoricalCrossentropy',  # 2
              metrics=['accuracy'])  # 3

saved_model_dir = 'keras_weights.h5'
model.save(saved_model_dir)
# To see the model summary in a tabular structure
model.summary()

face_data = 'train/'
required_shape = (160,160)
path = "keras_weights.h5"
model.load_weights(path)
face_detector = mtcnn.MTCNN()
encodes = []
encoding_dict = dict()
l2_normalizer = Normalizer('l2')

def normalize(img):
    mean, std = img.mean(), img.std()
    return (img - mean) / std

# Printing Training Variables
print('Number of trainable variables = {}'.format(len(model.trainable_variables)))

for face_names in os.listdir(face_data):
    person_dir = os.path.join(face_data,face_names)

    for image_name in os.listdir(person_dir):
        image_path = os.path.join(person_dir,image_name)
        print(image_name)
        img_BGR = cv2.imread(image_path)
        img_RGB = cv2.cvtColor(img_BGR, cv2.COLOR_BGR2RGB)

        x = face_detector.detect_faces(img_RGB)
        x1, y1, width, height = x[0]['box']
        x1, y1 = abs(x1) , abs(y1)
        x2, y2 = x1+width , y1+height
        face = img_RGB[y1:y2 , x1:x2]
        
        face = normalize(face)
        face = cv2.resize(face, required_shape)
        face_d = np.expand_dims(face, axis=0)
        encode = model.predict(face_d)[0]
        encodes.append(encode)

    if encodes:
        encode = np.sum(encodes, axis=0 )
        encode = l2_normalizer.transform(np.expand_dims(encode, axis=0))[0]
        encoding_dict[face_names] = encode

path = 'encodings/encodings.pkl'
with open(path, 'wb') as file:
    pickle.dump(encoding_dict, file,  pickle.HIGHEST_PROTOCOL)

confidence_t=0.99
recognition_t=0.5
required_size = (160,160)

def get_encode(face_encoder, face, size):
    face = normalize(face)
    face = cv2.resize(face, size)
    encode = face_encoder.predict(np.expand_dims(face, axis=0))[0]
    return encode
    
def load_pickle(path):
    with open(path, 'rb') as f:
        encoding_dict = pickle.load(f)
    return encoding_dict
def get_face(img, box):
    x1, y1, width, height = box
    x1, y1 = abs(x1), abs(y1)
    x2, y2 = x1 + width, y1 + height
    face = img[y1:y2, x1:x2]
    return face, (x1, y1), (x2, y2)

def detect(img ,detector,encoder,encoding_dict):
    img_rgb = cv2.cvtColor(np.float32(img), cv2.COLOR_BGR2RGB)
    results = detector.detect_faces(img_rgb)
    for res in results:
        if res['confidence'] < confidence_t:
            continue
        face, pt_1, pt_2 = get_face(img_rgb, res['box'])
        encode = get_encode(encoder, face, required_size)
        encode = l2_normalizer.transform(encode.reshape(1, -1))[0]
        name = 'unknown'

        distance = float("inf")
        for db_name, db_encode in encoding_dict.items():
            dist = cosine(db_encode, encode)
            if dist < recognition_t and dist < distance:
                name = db_name
                distance = dist
                
        if name == 'unknown':
            print('bukan')
        else:
            print(name)
    return img

img = cv2.imread('/content/train/chelsea/chelsea.jpg')
frame= detect(img , face_detector , model , encoding_dict)

